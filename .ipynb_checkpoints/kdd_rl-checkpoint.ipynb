{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'1': [0.55, 0.7], '2': [0, 0], '3': [0, 0], '4': [0, 0], '5': [0, 0]}]\n"
     ]
    }
   ],
   "source": [
    "policies = []\n",
    "policy = {}\n",
    "\n",
    "policy['1']=[.55,.7]\n",
    "policy['2']=[0,0]\n",
    "policy['3']=[0,0]\n",
    "policy['4']=[0,0]\n",
    "policy['5']=[0,0]\n",
    "\n",
    "policies.append(policy)\n",
    "print(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from netsapi.challenge import *\n",
    "from sys import exit, exc_info, argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from netsapi.challenge import *\n",
    "from sys import exit, exc_info, argv\n",
    "\n",
    "class CustomAgent:\n",
    "    def __init__(self, environment, popsize=10):\n",
    "        self.popsize=popsize\n",
    "        self.environment = environment\n",
    "        self.popsize = popsize\n",
    "\n",
    "            \n",
    "        self.episodes = []\n",
    "        self.scores = []\n",
    "        self.policies = []\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        try:\n",
    "            # select a set of random candidate solutions to be evaluated\n",
    "            policies = np.random.rand(self.popsize, self.environment.policyDimension)\n",
    "            rewards = self.environment.evaluateReward(policies)\n",
    "            best_policy = policies[np.argmax(rewards),:]\n",
    "        \n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            print(exc_info())\n",
    "            \n",
    "        return best_policy\n",
    "\n",
    "    def scoringFunction(self):\n",
    "        scores = []\n",
    "        for ii in range(10):\n",
    "            self.environment.reset()\n",
    "            finalresult = aself.generate()\n",
    "            self.policies.append(finalresult)\n",
    "            reward = self.environment.evaluateReward(finalresult)\n",
    "            self.scores.append(reward)\n",
    "            self.episodes.append(ii)\n",
    "\n",
    "        return np.mean(self.scores)/np.std(self.scores)\n",
    " \n",
    "        \n",
    "    def create_submissions(self, filename = 'my_submission.csv'):\n",
    "        labels = ['episode_no', 'reward', 'policy']\n",
    "        rewards = np.array(self.scores)\n",
    "        data = { 'episode_no': self.episodes, \n",
    "                 'rewards': rewards,\n",
    "                 'policy': self.policies,\n",
    "               }\n",
    "        submission_file = pd.DataFrame(data)\n",
    "        submission_file.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n",
      "1000  Evaluations Remaining\n",
      "990  Evaluations Remaining\n"
     ]
    }
   ],
   "source": [
    "env = ChallengeEnvironment(experimentCount = 1000)\n",
    "a = CustomAgent(env)\n",
    "a.scoringFunction()\n",
    "a.create_submissions(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
